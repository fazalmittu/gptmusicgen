{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb90d141",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\coolf\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm, trange\n",
    "import torch.nn.functional as F\n",
    "import csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eaa571f",
   "metadata": {},
   "source": [
    "# Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d051ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only want to use english songs\n",
    "\n",
    "lyrics = pd.read_csv('lyrics-data.csv')\n",
    "lyrics = lyrics[lyrics['language']=='en']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c508b4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only want to keep rap songs\n",
    "\n",
    "artists = pd.read_csv('artists-data.csv')\n",
    "artists = artists[(artists['Genres'].isin(['Rap']))]\n",
    "music_df = lyrics.merge(artists[['Artist', 'Genres', 'Link']], left_on='ALink', right_on='Link', how='inner')\n",
    "music_df = music_df.drop(columns=['ALink','SLink','Link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b32a0e16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SName</th>\n",
       "      <th>Lyric</th>\n",
       "      <th>language</th>\n",
       "      <th>Artist</th>\n",
       "      <th>Genres</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Killing Me Softly With His Song</td>\n",
       "      <td>Strumming my pain with his fingers\\nSinging my...</td>\n",
       "      <td>en</td>\n",
       "      <td>Fugees</td>\n",
       "      <td>Rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How Many Mics</td>\n",
       "      <td>Intro: Wyclef Jean\\nPick up your microphones\\n...</td>\n",
       "      <td>en</td>\n",
       "      <td>Fugees</td>\n",
       "      <td>Rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ready Or Not</td>\n",
       "      <td>Ready or not, here I come, you can't hide\\nGon...</td>\n",
       "      <td>en</td>\n",
       "      <td>Fugees</td>\n",
       "      <td>Rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vocab (LP Version)</td>\n",
       "      <td>Chorus\\nYou got the vocab\\nI got the vocab\\nYo...</td>\n",
       "      <td>en</td>\n",
       "      <td>Fugees</td>\n",
       "      <td>Rap</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Zealots</td>\n",
       "      <td>CLEF]\\nAnother MC lose his life tonight, lord\\...</td>\n",
       "      <td>en</td>\n",
       "      <td>Fugees</td>\n",
       "      <td>Rap</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             SName  \\\n",
       "0  Killing Me Softly With His Song   \n",
       "1                    How Many Mics   \n",
       "2                     Ready Or Not   \n",
       "3               Vocab (LP Version)   \n",
       "4                          Zealots   \n",
       "\n",
       "                                               Lyric language  Artist Genres  \n",
       "0  Strumming my pain with his fingers\\nSinging my...       en  Fugees    Rap  \n",
       "1  Intro: Wyclef Jean\\nPick up your microphones\\n...       en  Fugees    Rap  \n",
       "2  Ready or not, here I come, you can't hide\\nGon...       en  Fugees    Rap  \n",
       "3  Chorus\\nYou got the vocab\\nI got the vocab\\nYo...       en  Fugees    Rap  \n",
       "4  CLEF]\\nAnother MC lose his life tonight, lord\\...       en  Fugees    Rap  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dddc24d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2012, 5)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "801986d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# want to remove songs that are too long; token limit\n",
    "\n",
    "music_df = music_df[music_df['Lyric'].apply(lambda x: len(x.split(' ')) < 350)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7720eb2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(517, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "music_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "915713d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a very small test set to compare generated text with the reality\n",
    "test_set = music_df.sample(n = 50)\n",
    "music_df = music_df.loc[~music_df.index.isin(test_set.index)]\n",
    "\n",
    "#Reset the indexes\n",
    "test_set = test_set.reset_index()\n",
    "music_df = music_df.reset_index()\n",
    "\n",
    "#For the test set only, keep last 20 words in a new column, then remove them from original column\n",
    "test_set['True_end_lyrics'] = test_set['Lyric'].str.split().str[-20:].apply(' '.join)\n",
    "test_set['Lyric'] = test_set['Lyric'].str.split().str[:-20].apply(' '.join)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48f8a685",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = test_set.drop(columns=[\"index\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef78968e",
   "metadata": {},
   "source": [
    "# Tokenize Lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ede6450b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(df, truncate=False, gpt2_type=\"gpt2\", max_length=1024):\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(gpt2_type)\n",
    "    lyrics = []\n",
    "\n",
    "    for row in music_df['Lyric']:\n",
    "        lyrics.append(torch.tensor(\n",
    "            tokenizer.encode(f\"<|{df}|>{row[:max_length]}<|endoftext|>\")\n",
    "        ))  \n",
    "        \n",
    "    if truncate:\n",
    "        lyrics = lyrics[:20000]\n",
    "    \n",
    "    return lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f595353",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_lyrics = tokenizer(music_df[\"Lyric\"], truncate=True, gpt2_type=\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65b0f792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "467"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_lyrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5f6dab",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a60db8c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████████████████████████████████████████████████████████████| 548M/548M [00:25<00:00, 21.4MB/s]\n",
      "C:\\Users\\coolf\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\huggingface_hub\\file_download.py:127: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\coolf\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36d9b42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_tensor(new_tensor, packed_tensor, max_seq_len):\n",
    "    if packed_tensor is None:\n",
    "        return new_tensor, True, None\n",
    "    if new_tensor.size()[1] + packed_tensor.size()[1] > max_seq_len:\n",
    "        return packed_tensor, False, new_tensor\n",
    "    else:\n",
    "        packed_tensor = torch.cat([new_tensor, packed_tensor[:, 1:]], dim=1)\n",
    "        return packed_tensor, True, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d28dd6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(\n",
    "    dataset, \n",
    "    model, \n",
    "    tokenizer,\n",
    "    batch_size=16, \n",
    "    epochs=5, \n",
    "    lr=2e-5,\n",
    "    max_seq_len=400, \n",
    "    warmup_steps=200,\n",
    "    gpt2_type=\"gpt2\", \n",
    "    output_dir=\".\", \n",
    "    output_prefix=\"wreckgar\",\n",
    "    test_mode=False,\n",
    "    save_model_on_epoch=False,\n",
    "):\n",
    "    \n",
    "    acc_steps = 100\n",
    "#     device=torch.device(\"cuda\")\n",
    "#     model = model.cuda()\n",
    "    model.train()\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=warmup_steps, num_training_steps=-1\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
    "    loss=0\n",
    "    accumulating_batch_count = 0\n",
    "    input_tensor = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        print(f\"Training epoch {epoch}\")\n",
    "        print(loss)\n",
    "        for idx, entry in tqdm(enumerate(train_dataloader)):\n",
    "            (input_tensor, carry_on, remainder) = pack_tensor(entry, input_tensor, 768)\n",
    "\n",
    "            if carry_on and idx != len(train_dataloader) - 1:\n",
    "                continue\n",
    "\n",
    "#             input_tensor = input_tensor.to(device)\n",
    "            outputs = model(input_tensor, labels=input_tensor)\n",
    "            loss = outputs[0]\n",
    "            loss.backward()\n",
    "\n",
    "            if (accumulating_batch_count % batch_size) == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                model.zero_grad()\n",
    "\n",
    "            accumulating_batch_count += 1\n",
    "            input_tensor = None\n",
    "        if save_model_on_epoch:\n",
    "            torch.save(\n",
    "                model.state_dict(),\n",
    "                os.path.join(output_dir, f\"{output_prefix}-{epoch}.pt\"),\n",
    "            )\n",
    "            \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5c25b67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 0\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "467it [28:45,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 1\n",
      "tensor(3.9118, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "467it [29:46,  3.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 2\n",
      "tensor(3.8541, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "467it [28:48,  3.70s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 3\n",
      "tensor(3.7502, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "467it [28:57,  3.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training epoch 4\n",
      "tensor(3.2721, grad_fn=<NllLossBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "467it [37:03,  4.76s/it]\n"
     ]
    }
   ],
   "source": [
    "model = train(tokenized_lyrics, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "92fb5f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    prompt,\n",
    "    entry_count=10,\n",
    "    entry_length=30, # maximum number of words\n",
    "    top_p=0.8,\n",
    "    temperature=1.\n",
    "):\n",
    "    model.eval()\n",
    "    generated_num = 0\n",
    "    generated_list = []\n",
    "\n",
    "    filter_value = -float(\"Inf\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for entry_idx in trange(entry_count):\n",
    "\n",
    "            entry_finished = False\n",
    "            generated = torch.tensor(tokenizer.encode(prompt)).unsqueeze(0)\n",
    "#             print(generated.size())\n",
    "#             generated_tokens = torch.empty((1,0))\n",
    "\n",
    "            for i in range(entry_length):\n",
    "                outputs = model(generated, labels=generated)\n",
    "                loss, logits = outputs[:2]\n",
    "                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n",
    "\n",
    "                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
    "                cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
    "\n",
    "                sorted_indices_to_remove = cumulative_probs > top_p\n",
    "                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n",
    "                    ..., :-1\n",
    "                ].clone()\n",
    "                sorted_indices_to_remove[..., 0] = 0\n",
    "\n",
    "                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n",
    "                logits[:, indices_to_remove] = filter_value\n",
    "\n",
    "                next_token = torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)\n",
    "                generated = torch.cat((generated, next_token), dim=1)\n",
    "#                 generated = next_token\n",
    "#                 print(f\"generated: {generated_tokens}\")\n",
    "\n",
    "                if next_token in tokenizer.encode(\"<|endoftext|>\"):\n",
    "                    entry_finished = True\n",
    "\n",
    "                if entry_finished:\n",
    "                    generated_num = generated_num + 1\n",
    "\n",
    "                    output_list = list(generated.squeeze().numpy())\n",
    "                    output_text = tokenizer.decode(output_list)\n",
    "                    print(generated_list)\n",
    "                    generated_list.append(output_text)\n",
    "                    break\n",
    "            \n",
    "            if not entry_finished:\n",
    "                output_list = list(generated.squeeze().numpy())\n",
    "                output_text = f\"{tokenizer.decode(output_list)}\" \n",
    "                print(generated_list)\n",
    "                generated_list.append(output_text)\n",
    "                \n",
    "#     return generated_list\n",
    "    return output_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "1f98eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Function to generate multiple sentences. Test data should be a dataframe\n",
    "# def text_generation(test_data):\n",
    "#     generated_lyrics = []\n",
    "#     for i in range(len(test_data)):\n",
    "#         x = generate(model, tokenizer, test_data['Lyric'][i], entry_count=1)\n",
    "#         generated_lyrics.append(x)\n",
    "#     return generated_lyrics\n",
    "\n",
    "# # Run the functions to generate the lyrics\n",
    "# generated_lyrics = text_generation(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "5c9c8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Loop to keep only generated text and add it as a new column in the dataframe\n",
    "# my_generations = []\n",
    "\n",
    "# for i in range(len(generated_lyrics)):\n",
    "# #     a = test_set['Lyric'][i].split()[-30:] #Get the matching string we want (30 words)\n",
    "# #     b = ' '.join(a)\n",
    "#     c = generated_lyrics[i] #Get all that comes after the matching string\n",
    "#     my_generations.append(c.split(b)[-1])\n",
    "\n",
    "# test_set['Generated_lyrics'] = my_generations\n",
    "\n",
    "\n",
    "# #Finish the sentences when there is a point, remove after that\n",
    "# final = []\n",
    "\n",
    "# for i in range(len(test_set)):\n",
    "#     to_remove = test_set['Generated_lyrics'][i].split('.')[-1]\n",
    "#     final.append(test_set['Generated_lyrics'][i].replace(to_remove,''))\n",
    "\n",
    "# test_set['Generated_lyrics'] = final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "d38afb67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8641400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drake_prompt = \"You know good and well that you don't want a problem like that. You gon' make someone around me catch a body like that.\"\n",
    "prompt = \"I might be too strung out on compliments, Overdosed on confidence, Started not to give a fuck and stopped fearin' the consequence, Drinkin' every night because we drink to my accomplishments, Faded way too long, I'm floatin' in and out of consciousness, And they sayin' I'm back, I'd agree with that, I just take my time with all this shit, I still believe in that, I had someone tell me I fell off, ooh, I needed that, And they want to see me pick back up, well, where'd I leave it at?, I know I exaggerated things, now I got it like that, Tuck my napkin in my shirt 'cause I'm just mobbin' like that\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "629d51b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:06<00:00,  6.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "new_lyric = generate(model, tokenizer, drake_prompt, entry_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "4a66a7df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'You know good and well that you don\\'t want a problem like that. You gon\\' make someone around me catch a body like that. And you know what I mean?\"\\n\\n\"What? What is the point?\"\\n\\n\"But you know, maybe one day when you\\'re'"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_lyric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a589646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
